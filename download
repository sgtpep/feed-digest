#!/usr/bin/env python
import datetime
import os
import re
import socket
import sqlite3
import time
from multiprocessing import Pool

import feedparser

import config

def parse_feed(feed_url):
    result = feedparser.parse(feed_url, agent="Mozilla/5.0 (X11; Linux x86_64; rv:37.0) Gecko/20100101 Firefox/37.0")

    return feed_url, result.feed, result.entries

if __name__ == '__main__':
    added = datetime.datetime.now()
    added -= datetime.timedelta(0, added.second, added.microsecond)
    if not os.path.exists(config.DB_PATH):
        added -= datetime.timedelta(seconds=config.GROUP_INTERVAL)

    connection = sqlite3.connect(config.DB_PATH)
    cursor = connection.cursor()

    cursor.executescript("""
    CREATE TABLE IF NOT EXISTS entries (
        feed_number INTEGER, feed_url TEXT, feed_title TEXT, number INTEGER, url TEXT, title TEXT, added TIMESTAMP, published TIMESTAMP,
        PRIMARY KEY (feed_url, url)
    );
    CREATE INDEX IF NOT EXISTS entries_added ON entries (added);
    CREATE INDEX IF NOT EXISTS entries_order ON entries (feed_number, added, number);
    CREATE INDEX IF NOT EXISTS entries_stats ON entries (feed_url, published);
    """)

    feed_lines = open(config.URLS_PATH).read().strip().splitlines()
    feed_urls = [i.split(None, 1)[0] for i in feed_lines if not i.lstrip().startswith('#')]

    socket.setdefaulttimeout(config.SOCKET_TIMEOUT)

    pool = Pool(config.NUMBER_OF_PROCESSES)
    results = pool.map(parse_feed, feed_urls)
    pool.close()
    pool.join()
    
    rows = []
    url_base_regex = re.compile(r"^[^:]+")
    for feed_number, (feed_url, feed, entries) in enumerate(results):
        feed_title = feed.get('title') or feed_url
        feed_url_base = url_base_regex.sub('', feed_url)
        for number, entry in enumerate(entries):
            url = entry.get('link', '')
            title = entry.get('title') or url
            url_base = url_base_regex.sub('', url)
            result = cursor.execute("UPDATE entries SET title = ? WHERE feed_url IN (?, ?) AND url IN (?, ?)", (title, 'http' + feed_url_base, 'https' + feed_url_base, 'http' + url_base, 'https' + url_base))
            if result.rowcount:
                continue

            published = entry.get('published_parsed') or entry.get('updated_parsed') or entry.get('created_parsed')
            published = datetime.datetime.fromtimestamp(time.mktime(published)) if published else added
            rows.append((feed_number, feed_url, feed_title, number, url, title, added, published))

    cursor.executemany("INSERT OR REPLACE INTO entries VALUES (?, ?, ?, ?, ?, ?, ?, ?)", rows)
    connection.commit()
